{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"a8eZ6PWTHriw"},"outputs":[],"source":["!pip install -q datasets jiwer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pkSzlRJq68tH"},"outputs":[],"source":["!pip install transformers -U"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"es3Z7toVq6Z7"},"outputs":[],"source":["!pip install accelerate -U"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NlQBhTOss8Pn"},"outputs":[],"source":["!pip install transformers[torch] -U"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rZcEngDtSsSB"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8GQpF6Bvsow"},"outputs":[],"source":["Path = '/content/drive/MyDrive/ImgToLatex/First_15K_of_30K_LCDataset.xlsx'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KkHqJw-W9Abl"},"outputs":[],"source":["import pandas as pd\n","\n","SourceExcelFile = pd.read_excel(Path)\n","\n","SourceExcelFile.head()"]},{"cell_type":"markdown","metadata":{"id":"qJlVYVal9Ojy"},"source":["We split up the data into training + testing, using sklearn's `train_test_split` function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qLVT1TPN8Nt"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","train_df, test_df = train_test_split(SourceExcelFile, test_size=0.3)\n","\n","# we reset the indices to start from zero\n","train_df.reset_index(drop=True, inplace=True)\n","test_df.reset_index(drop=True, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"fwlEBh6B9RTE"},"source":["Each element of the dataset should return 2 things:\n","* `pixel_values`, which serve as input to the model.\n","* `labels`, which are the `input_ids` of the corresponding text in the image.\n","\n","We use `TrOCRProcessor` to prepare the data for the model. `TrOCRProcessor` is actually just a wrapper around a `ViTFeatureExtractor` (which can be used to resize + normalize images) and a `RobertaTokenizer` (which can be used to encode and decode text into/from `input_ids`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qTF-XFVZLfrq"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from PIL import Image\n","\n","def Loader(df, maxe):\n","  ImgSourceDir = '/content/drive/My Drive/ImgToLatex/LatexImages/'\n","\n","  FileNames = list(df.columns[2 : 6])\n","\n","  FinalDF = pd.DataFrame(columns = ['Image', 'Text'])\n","  NotFoundFiles = []\n","\n","  count = 0\n","\n","  for i in range(len(df)):\n","\n","    DirectoryName = df.iloc[i]['DirectoryName']\n","\n","    for FileDpi in FileNames:\n","\n","      try:\n","        text = df.iloc[i]['Latex Code']\n","        ImgFile = ImgSourceDir + DirectoryName + '/' + df.iloc[i][FileDpi] + '.png'\n","        img = Image.open(ImgFile)\n","\n","        newRow = pd.DataFrame({\"Image\": [ImgFile], \"Text\": [text]})\n","        FinalDF = pd.concat([FinalDF, newRow], ignore_index = True)\n","        count += 1\n","        print(len(FinalDF))\n","        if count == maxe:\n","          return(FinalDF, NotFoundFiles)\n","      except FileNotFoundError:\n","        NotFoundFiles.append(df.iloc[i][FileDpi])\n","  return(FinalDF, NotFoundFiles)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H_7fLw4qGGCt"},"outputs":[],"source":["train, files = Loader(train_df, 10000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQ8xxgghz8Jv"},"outputs":[],"source":["#Dont Run This\n","\n","train.to_csv('/content/drive/My Drive/ImgToLatex/train.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CG5L588Q14Xu"},"outputs":[],"source":["#Run from here..........\n","\n","import pandas as pd\n","train = pd.read_csv('/content/drive/My Drive/ImgToLatex/First10000/train.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J2uS_haSGGzi"},"outputs":[],"source":["eval, efiles = Loader(test_df, 2000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SmyAZjrQ0KVi"},"outputs":[],"source":["#Dont Run This\n","\n","eval.to_csv('/content/drive/My Drive/ImgToLatex/eval.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qvz__Jrv2Fcf"},"outputs":[],"source":["eval = pd.read_csv('/content/drive/My Drive/ImgToLatex/First10000/eval.csv')"]},{"cell_type":"markdown","metadata":{"id":"yzL7C60c-v-B"},"source":["Let's initialize the training and evaluation datasets:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1AT_VPbsv02X"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from PIL import Image\n","\n","class IAMDataset(Dataset):\n","    def __init__(self, root_dir, df, processor, max_target_length=256):\n","        self.root_dir = root_dir\n","        self.df = df\n","        self.processor = processor\n","        self.max_target_length = max_target_length\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        # get file name + text\n","        file_name = self.df['Image'][idx]\n","        text = self.df['Text'][idx]\n","        # prepare image (i.e. resize + normalize)\n","        image = Image.open(file_name).convert(\"RGB\")\n","        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n","        # add labels (input_ids) by encoding the text\n","        labels = self.processor.tokenizer(text,\n","                                          padding=\"max_length\",\n","                                          max_length=self.max_target_length).input_ids\n","        # important: make sure that PAD tokens are ignored by the loss function\n","        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n","\n","        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n","        return encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KIa78c2W8uT9"},"outputs":[],"source":["from transformers import TrOCRProcessor\n","from transformers import convert_slow_tokenizer\n","\n","ImgFilePath = '/content/drive/MyDrive/LatexDataset_02_01_2024/LatexImages'\n","\n","processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RmpHrLIsZxYB"},"outputs":[],"source":["train_dataset = IAMDataset(root_dir = ImgFilePath, df = train, processor = processor)\n","eval_dataset = IAMDataset(root_dir = ImgFilePath, df = eval, processor=processor)"]},{"cell_type":"markdown","metadata":{"id":"7p8JfQrx-6EM"},"source":["Let's verify an example from the training dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ygzo3ekYFoAP"},"outputs":[],"source":["encoding = train_dataset[0]\n","for k,v in encoding.items():\n","  print(k, v.shape)"]},{"cell_type":"markdown","metadata":{"id":"lN-3pf6T_uRe"},"source":["We can also check the original image and decode the labels:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzgOFgD4_7Kw"},"outputs":[],"source":["#Dont Run This\n","\n","image = Image.open(train_dataset.root_dir + train_df['DPI-200'][0]).convert(\"RGB\")\n","image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vMtfkDia-8tQ"},"outputs":[],"source":["labels = encoding['labels']\n","labels[labels == -100] = processor.tokenizer.pad_token_id\n","label_str = processor.decode(labels, skip_special_tokens=True)\n","print(label_str)"]},{"cell_type":"markdown","metadata":{"id":"XxU7TfoYBvg0"},"source":["## Train a model\n","\n","Here, we initialize the TrOCR model from its pretrained weights. Note that the weights of the language modeling head are already initialized from pre-training, as the model was already trained to generate text during its pre-training stage. Refer to the paper for details."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRhvTRrGBIfy"},"outputs":[],"source":["from transformers import VisionEncoderDecoderModel\n","\n","model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")"]},{"cell_type":"markdown","metadata":{"id":"UqNELu3cQix5"},"source":["Importantly, we need to set a couple of attributes, namely:\n","* the attributes required for creating the `decoder_input_ids` from the `labels` (the model will automatically create the `decoder_input_ids` by shifting the `labels` one position to the right and prepending the `decoder_start_token_id`, as well as replacing ids which are -100 by the pad_token_id)\n","* the vocabulary size of the model (for the language modeling head on top of the decoder)\n","* beam-search related parameters which are used when generating text."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sNNT1XS_CMgl"},"outputs":[],"source":["# set special tokens used for creating the decoder_input_ids from the labels\n","model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n","model.config.pad_token_id = processor.tokenizer.pad_token_id\n","# make sure vocab size is set correctly\n","model.config.vocab_size = model.config.decoder.vocab_size\n","\n","# set beam search parameters\n","model.config.eos_token_id = processor.tokenizer.sep_token_id\n","model.config.max_length = 64\n","model.config.early_stopping = True\n","model.config.no_repeat_ngram_size = 3\n","model.config.length_penalty = 2.0\n","model.config.num_beams = 4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yoXD3_P10DD4"},"outputs":[],"source":["from datasets import load_metric\n","\n","cer_metric = load_metric(\"cer\")"]},{"cell_type":"markdown","metadata":{"id":"8G0R0sPFvfqT"},"source":["The compute_metrics function takes an `EvalPrediction` (which is a NamedTuple) as input, and should return a dictionary. The model will return an EvalPrediction at evaluation, which consists of 2 things:\n","* predictions: the predictions by the model.\n","* label_ids: the actual ground-truth labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y36AcnvP0OZw"},"outputs":[],"source":["def compute_metrics(pred):\n","    labels_ids = pred.label_ids\n","    pred_ids = pred.predictions\n","\n","    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n","    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n","    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n","\n","    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"cer\": cer}"]},{"cell_type":"markdown","metadata":{"id":"nV6KY53xvOgC"},"source":["We will evaluate the model on the Character Error Rate (CER), which is available in HuggingFace Datasets (see [here](https://huggingface.co/metrics/cer))."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DVJY9ExoG-b4"},"outputs":[],"source":["import os\n","from google.colab import drive\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, default_data_collator\n","import accelerate\n","\n","# Specify the directory path in your Google Drive\n","drive_output_dir = \"/content/drive/MyDrive/ImgToLatex/TrOcr\"\n","\n","# Set up training arguments\n","training_args = Seq2SeqTrainingArguments(\n","    num_train_epochs = 25,\n","    predict_with_generate=True,\n","    evaluation_strategy=\"steps\",\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    fp16=True,\n","    output_dir=drive_output_dir,\n","    logging_steps=2,\n","    save_steps=1000,\n","    eval_steps=200,\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    tokenizer=processor.image_processor,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    data_collator=default_data_collator,\n",")\n","\n","try:\n","    # Train the model\n","    trainer.train()\n","except (KeyboardInterrupt, RuntimeError) as e:\n","    # Handle keyboard interrupt and runtime disconnect errors\n","    print(f\"Error during training: {e}\")\n","finally:\n","    # Save the model even if there's an interruption or error\n","    print(\"Saving model...\")\n","    model.save_pretrained(drive_output_dir)\n","    #training_args.save_model_args(drive_output_dir)\n","    print(\"Model saved in Google Drive.\")\n"]},{"cell_type":"markdown","metadata":{"id":"q8tduj2fBGQK"},"source":["## Inference\n","\n","Note that after training, you can easily load the model using the .`from_pretrained(output_dir)` method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nrUSrfGBHZo5"},"outputs":[],"source":["TestPath = '/content/drive/My Drive/ImgToLatex/LatexImages/1-1500/latex_image_750_dpi_200_normal.png'\n","\n","image = Image.open(TestPath).convert(\"RGB\")\n","\n","image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGYOuHSOIo4B"},"outputs":[],"source":["#Dont Run This\n","\n","pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n","\n","generated_ids = model.generate(pixel_values)\n","generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","\n","print(generated_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Kmpq2QWuzW_"},"outputs":[],"source":["Model = VisionEncoderDecoderModel.from_pretrained(drive_output_dir)\n","\n","pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n","\n","generated_ids = Model.generate(pixel_values)\n","generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","\n","print(generated_text)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4","private_outputs":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}